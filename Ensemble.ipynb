{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMjm+PxmlOjW+0lnGQRcOAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mendesdyl/mendesdyl/blob/main/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating an ensemble of states to show differences in the clustering "
      ],
      "metadata": {
        "id": "PEMSwxDD62bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  After simulating replicates of magainin, a protien from and african frog, KMeans clustering was applied to a large group of all simulations. These clusters have been probed for bound states through distance to the bilayer, secondary structure, alpha carbon distances, and the final part of characterinzing the clusters is the visual ensemble created via this code. Enjoy :) DylanMendes"
      ],
      "metadata": {
        "id": "fbrCC_zw0pys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages and Mount Google Drive"
      ],
      "metadata": {
        "id": "UGOzshSZSnyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the packages here and installing pyemma because it was not working on DT2 (DeepThought2(HPC))"
      ],
      "metadata": {
        "id": "YSzKp-N5RY2e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TVbidCNarKe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#!pip install pyemma\n",
        "#import pyemma\n",
        "!pip install mdtraj\n",
        "import mdtraj as md\n",
        "import pprint\n",
        "from six.moves import xrange\n",
        "import re\n",
        "import os\n",
        "import shutil\n",
        "import argparse\n",
        "import time\n",
        "import gzip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The whole reason I did the ensemble of states on this platform was that I would be able to use pyemma but the way Min-Kang had it implemented I did not need to use pyemma. Therefore, pyemma is commented out, MDTraj was used instead for all the calculations. "
      ],
      "metadata": {
        "id": "aII2a31pwRiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Google Drive to access the files uploaded from DeepThought2"
      ],
      "metadata": {
        "id": "MUIUvngNU69I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# /content/gdrive/MyDrive/ensemble/\n",
        "# This is the path once the drive is mounted"
      ],
      "metadata": {
        "id": "92UVfBjvV9S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run the ensemble of states (more defenition below) "
      ],
      "metadata": {
        "id": "ZkM3NJXaRpM8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert the files from dcd and pdb to h5 format in order to process them as one large clustering"
      ],
      "metadata": {
        "id": "9KlX85vQu0FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dyn = [150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 92, 162, 149, 161, 131, 137, 157, 158]\n",
        "s=0\n",
        "o=1\n",
        "for i in range(0, 19):\n",
        "    pdbFile = i + 1\n",
        "    for d in range(1,int(dyn[i]+1)):\n",
        "        cmd = \"mdconvert -o /content/gdrive/MyDrive/ensemble/comb/{0}.h5 -t /content/gdrive/MyDrive/ensemble/dcd_a_ca/a_ca{1}.pdb /content/gdrive/MyDrive/ensemble/dcd_a_ca/a_ca_dyn{2}.dcd\".format(s,pdbFile,o)\n",
        "        os.system(cmd)\n",
        "        s = s + 1\n",
        "        o = o + 1\n",
        "\n",
        "#this will create new .h5 files that include topology because that caused a ton of errors "
      ],
      "metadata": {
        "id": "dCrJnYBaDl5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute the top 50 least RMSD frames into new dcd and pdb files "
      ],
      "metadata": {
        "id": "ZG-v9_J3vC27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f=open('/content/gdrive/MyDrive/ensemble/cluster_id_s9.dat','r')\n",
        "##Edit for your simulation\n",
        "inds = []\n",
        "y_m_t =[]\n",
        "lines=f.readlines()\n",
        "#lines= lines[FLine:LLine]\n",
        "for x in lines:\n",
        "    inds.append(int(x.split()[0]))\n",
        "    y_m_t.append(int(x.split()[1]))\n",
        "f.close()\n",
        "y_m_t = np.array(y_m_t)\n",
        "inds = np.array(inds)\n",
        "inds=inds -1\n",
        "\n",
        "#frame numbers of each cluster store in shapshots\n",
        "#the frames of each cluster may be different\n",
        "snapshots = []\n",
        "unique = sorted(np.unique(y_m_t))\n",
        "for i in range(len(np.unique(y_m_t))):\n",
        "    snapshots.append(inds[y_m_t == i])\n",
        "\n",
        "ml_samples = snapshots \n",
        "\n",
        "dyn = [150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 92, 162, 149, 161, 131, 137, 157, 158]\n",
        "ts = [10, 10, 10, 10, 10, 10, 10, 60, 60, 60, 60, 600, 200, 200, 200, 600, 600, 200, 200]\n",
        "##Edit for your trajectories with length of each trajectory in [dyn] and times step per dcd in [ts]\n",
        "\n",
        "cluster1 = []\n",
        "cluster2 = []\n",
        "cluster3 = []\n",
        "cluster4 = []\n",
        "cluster5 = []\n",
        "cluster6 = []\n",
        "cluster7 = []\n",
        "cluster8 = []\n",
        "cluster9 = []\n",
        "##Edit the location of your combined now h5 trajectories so they can be loaded in the for loop below \n",
        "for row in ml_samples[0]:\n",
        "  cluster1.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[1]:\n",
        "  cluster2.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[2]:\n",
        "  cluster3.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[3]:\n",
        "  cluster4.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[4]:\n",
        "  cluster5.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[5]:\n",
        "  cluster6.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[6]:\n",
        "  cluster7.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[7]:\n",
        "  cluster8.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "for row in ml_samples[8]:\n",
        "  cluster9.append('/content/gdrive/MyDrive/ensemble/comb/'+str(row)+'.h5')\n",
        "\n",
        "##Edit there are 9 simualtions in the clustering done hence the range(1, 10)\n",
        "for c in range(1,10):\n",
        "  listt = \"cluster\"+str(c)\n",
        "  cluster = md.load(eval(listt))\n",
        "  avg = cluster.xyz.mean(axis=0)\n",
        "  cluster.xyz = avg\n",
        "  ff = \"/content/gdrive/MyDrive/ensemble/final2/avg{0}.h5\".format(c)\n",
        "  f = md.formats.HDF5TrajectoryFile(ff, 'w')\n",
        "  f.write(cluster.xyz)\n",
        "  f.close()\n",
        "  cluster = md.load(eval(listt))\n",
        "  tmp = md.load('/content/gdrive/MyDrive/ensemble/final2/avg'+str(c)+'.h5')\n",
        "  rm = md.rmsd(cluster, tmp, 0)\n",
        "  indices = rm.argsort()[:50]\n",
        "  np.savetxt(\"/content/gdrive/MyDrive/ensemble/out/clusterFrames{0}.csv\".format(c),indices, delimiter =\" \",  fmt ='% s')\n",
        "  name='/content/gdrive/MyDrive/ensemble/final/cluster_'+str(c)+'.dcd'\n",
        "  namepdb = '/content/gdrive/MyDrive/ensemble/final/cluster_'+str(c)+'.pdb'\n",
        "  cluster[indices].save_dcd(name)\n",
        "  cluster[indices[1]].save_pdb(namepdb)\n",
        "##Edit you have to change the location of the files but the code should work on its own "
      ],
      "metadata": {
        "id": "L6mkyeEIRhe8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}